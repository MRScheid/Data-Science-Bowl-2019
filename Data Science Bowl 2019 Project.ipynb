{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# matplotlib and seaborn for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import cross_val_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\micha\\Desktop\\Kaggle - Data Science Bowl 2019'\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train = pd.read_csv('train.csv')#, index_col='timestamp', parse_dates=True) #,usecols=keep_cols)\n",
    "train_labels = pd.read_csv('train_labels.csv')\n",
    "\n",
    "# Keep only the following columns in the test set to save RAM on the Kaggle Kernel\n",
    "keep_cols1 = ['installation_id','world','timestamp','type','game_time','game_session',]\n",
    "\n",
    "# Load the test data\n",
    "test = pd.read_csv('test.csv', usecols=keep_cols1) # nrows =100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train data (11341042, 11)\n",
      "Size of train_labels data (17690, 7)\n",
      "Size of specs data (386, 3)\n",
      "Size of test data (1156414, 6)\n"
     ]
    }
   ],
   "source": [
    "print('Size of train data', train.shape)\n",
    "print('Size of train_labels data', train_labels.shape)\n",
    "print('Size of specs data', specs.shape)\n",
    "print('Size of test data', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to reduce the DF size\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 778.73 Mb (18.2% reduction)\n",
      "Mem. usage decreased to  0.49 Mb (48.2% reduction)\n",
      "Mem. usage decreased to  0.01 Mb (0.0% reduction)\n",
      "Mem. usage decreased to 48.53 Mb (8.3% reduction)\n",
      "Wall time: 312 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## REducing memory\n",
    "train = reduce_mem_usage(train)\n",
    "train_labels = reduce_mem_usage(train_labels)\n",
    "specs = reduce_mem_usage(specs)\n",
    "test = reduce_mem_usage(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11341042 entries, 0 to 11341041\n",
      "Data columns (total 11 columns):\n",
      "event_id           object\n",
      "game_session       object\n",
      "timestamp          object\n",
      "event_data         object\n",
      "installation_id    object\n",
      "event_count        int16\n",
      "event_code         int16\n",
      "game_time          int32\n",
      "title              object\n",
      "type               object\n",
      "world              object\n",
      "dtypes: int16(2), int32(1), object(8)\n",
      "memory usage: 778.7+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17690 entries, 0 to 17689\n",
      "Data columns (total 7 columns):\n",
      "game_session       17690 non-null object\n",
      "installation_id    17690 non-null object\n",
      "title              17690 non-null object\n",
      "num_correct        17690 non-null int8\n",
      "num_incorrect      17690 non-null int8\n",
      "accuracy           17690 non-null float16\n",
      "accuracy_group     17690 non-null int8\n",
      "dtypes: float16(1), int8(3), object(3)\n",
      "memory usage: 501.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 386 entries, 0 to 385\n",
      "Data columns (total 3 columns):\n",
      "event_id    386 non-null object\n",
      "info        386 non-null object\n",
      "args        386 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 9.2+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1156414 entries, 0 to 1156413\n",
      "Data columns (total 6 columns):\n",
      "game_session       1156414 non-null object\n",
      "timestamp          1156414 non-null object\n",
      "installation_id    1156414 non-null object\n",
      "game_time          1156414 non-null int32\n",
      "type               1156414 non-null object\n",
      "world              1156414 non-null object\n",
      "dtypes: int32(1), object(5)\n",
      "memory usage: 48.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()\n",
    "train_labels.info()\n",
    "specs.info()\n",
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Game          572260\n",
       "Activity      466274\n",
       "Assessment    102627\n",
       "Clip           15253\n",
       "Name: type, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "7b728c89    21237\n",
       "5a6f3de8    18279\n",
       "090fe325    13758\n",
       "77d5414c    13231\n",
       "ce08e98b    12416\n",
       "            ...  \n",
       "69164a28        1\n",
       "5d3d2ce3        1\n",
       "27e272e5        1\n",
       "d1e3bd8c        1\n",
       "a87db7ff        1\n",
       "Name: installation_id, Length: 1000, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MAGMAPEAK       511291\n",
       "TREETOPCITY     332295\n",
       "CRYSTALCAVES    311387\n",
       "NONE              1441\n",
       "Name: world, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Explore data and see if there are any out of place values, missing values etc.\n",
    "display(test['type'].value_counts())\n",
    "display(test['installation_id'].value_counts())\n",
    "display(test['world'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing data\n"
     ]
    }
   ],
   "source": [
    "# Check for missing data\n",
    "assert pd.notna(train).all().all()\n",
    "assert pd.notna(train_labels).all().all()\n",
    "assert pd.notna(test).all().all()\n",
    "assert pd.notna(specs).all().all()\n",
    "print('No missing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (<ipython-input-119-a7c4e5fe773d>, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-119-a7c4e5fe773d>\"\u001b[1;36m, line \u001b[1;32m42\u001b[0m\n\u001b[1;33m    level=['installation_id','world','game_session']) \\ # 'assessment_id',\u001b[0m\n\u001b[1;37m                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "# train_labels['assessment_id'] = range(1,len(train_labels)+1)\n",
    "\n",
    "# Merge train_labels info with event data info to look at accuracy groups with respect to\n",
    "# different event info\n",
    "train_order = pd.merge_ordered(left = train, right = train_labels, \n",
    "                 on = ['installation_id','game_session'], how='left')\n",
    "train_order.sort_index()\n",
    "\n",
    "\n",
    "### Fill in missing accuracy group values for all events and\n",
    "### set index to variables that dataframe will be grouped by\n",
    "# First set index and sort chronologically within an \n",
    "#   [installation_id, world] pair with timestamp in index\n",
    "train_time = train_order.set_index(['installation_id','world','timestamp'])\n",
    "\n",
    "# Delete the train_order object to conserve RAM \n",
    "del train_order\n",
    "\n",
    "train_time = train_time.sort_index()\n",
    "\n",
    "# Backfill NaN values of events around assessments, bounded by installation_id and world \n",
    "train_time = train_time.groupby(level=['installation_id','world']).bfill()\n",
    "\n",
    "train_time.info()\n",
    "\n",
    "# # 'Clip' type activties don't have a gametime, so we need to fill in a time.  Assume about 2min / clip.\n",
    "train_time_clips = train_time[train_time['type'] == 'Clip'].replace(0,120000)['game_time']\n",
    "train_time.loc[train_time['type'] == 'Clip','game_time'] = train_time_clips \n",
    "\n",
    "# Then fill the rest of the events with 0 for those events \n",
    "# not associated with installation_ids or game_sessions where an assessment took place.  \n",
    "train_time = train_time.fillna(0)\n",
    "\n",
    "# Add accuracy_group and type to index so you can do a multi-level sort\n",
    "train_time = train_time.set_index(['game_session','accuracy_group','type'],append=True) # ,'assessment_id'\n",
    "\n",
    "# Auto-dispatch the sum aggregation on multi-level group using the groupby method\n",
    "# Essentially sum the amount of time for each type of gameplay for a particular installation_id ->\n",
    "# world -> accuracy_group -> type of event to get an estimate of time spent in \n",
    "# each type of activity\n",
    "train_time_agg = train_time.groupby( \\\n",
    "                 level=['installation_id','world','game_session']) \\ # 'assessment_id',\n",
    "                 .last()['game_time']\n",
    "\n",
    "train_time_agg = train_time['game_time'].groupby(level=['installation_id','world',\\\n",
    "                                                        'type','accuracy_group'])\\ # 'assessment_id',\n",
    "                                                        .agg(game_time='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now unstack the type column to get counts for each type of activity within each \n",
    "# [installation_id, world] pair\n",
    "train_time = train_time_agg.unstack('type')\n",
    "\n",
    "# Fill all those gameplay times that are empty with zeros so that the next step,\n",
    "# the cumulative sum below, executes correctly. \n",
    "train_time = train_time.fillna(0)\n",
    "\n",
    "# Calculate the cumulative sum of game time at assessment time in each type of gameplay for each \n",
    "# [installation_id world] pair\n",
    "train_time = train_time['game_time'].groupby(level=['installation_id','world'])\\\n",
    "                                        .transform(pd.DataFrame.cumsum)\n",
    "\n",
    "# Delete the train_time_agg object to conserve RAM on the Kaggle kernel\n",
    "del train_time_agg\n",
    "\n",
    "# Fill all [installation_id, world] rows that are missing an activity count with zeros\n",
    "train_time = train_time.fillna(0)\n",
    "\n",
    "# Now move the accuracy group out of the index to look at \n",
    "# correlation with type of activity counts\n",
    "train_time.reset_index(level=3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now look at the time spent in each type of gameplay separated by \n",
    "# accuracy_group.  \n",
    "sns.pairplot(train_time_scale,\n",
    "             kind=\"scatter\", \n",
    "             hue='accuracy_group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows a clear relationship between the cumulative amount of time spent on Activity and Clip type gameplays and the accuracy group of the child on the assessment.  Let's quantify this by performing a correlation.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corr = train_time_scale.corr()[['accuracy_group']]\n",
    "display(train_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, both Activity and Clip cumulative gameplay time are correlated with the accuracy_group.  Let's now build a model using these features and see how we do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to build models with the gameplay time features\n",
    "# Create arrays for the features and the response variable\n",
    "y = train_time['accuracy_group'].values\n",
    "X = train_time.drop(['accuracy_group','Assessment'], axis=1).values\n",
    "\n",
    "del train_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a k-NN classifier with 4 neighbors, one for each accuracy_group\n",
    "knn = KNeighborsClassifier(n_neighbors=4)\n",
    "\n",
    "# Fit the classifier to the data\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Its important to use binary mode \n",
    "knnPickle = open('knn_pickle_file', 'wb') \n",
    "\n",
    "# source, destination \n",
    "pickle.dump(knn, knnPickle)                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "knn = pickle.load(open('knn_pickle_file', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the accuracy\n",
    "print(knn.score(X_test, y_test))\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "cv_scores = cross_val_score(knn, X, y, cv = 5)\n",
    "\n",
    "# Print the 5-fold cross-validation scores\n",
    "print(cv_scores)\n",
    "\n",
    "print(\"Average 5-Fold CV Score: {}\".format(np.mean(cv_scores)))\n",
    "(len(y_test) - sum(y_pred != y_test))/ len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that with just 3 features of cumulative gameplay time in each type of activity (excluding assessment time), we can get approximately 75-80% accuracy in predicting the child's performance on assessments.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now format and process the test set to make predictions\n",
    "\n",
    "### Fill in missing accuracy group values for all events and\n",
    "### set index to variables that dataframe will be grouped by\n",
    "# First set index and sort chronologically within an \n",
    "#   [installation_id, world] pair with timestamp in index\n",
    "test_time = test.set_index(['installation_id','world','timestamp'])\n",
    "\n",
    "# Add a column, 'assessment_id', that keeps track of assessment \n",
    "# chronologically for each installation_id\n",
    "test_time.loc[test_time['type']=='Assessment','assessment_id'] = \\\n",
    "                                               range(1,len(test_time[test_time['type']=='Assessment'])+1)\n",
    "\n",
    "# del test\n",
    "\n",
    "test_time = test_time.sort_index()\n",
    "\n",
    "# Backfill NaN values of events that took place before assessments, these\n",
    "# will be the events that impacted performance on those assessments.\n",
    "test_time = test_time.groupby(level=['installation_id','world']).bfill()\n",
    "\n",
    "# # 'Clip' type activities don't have a gametime, so we need to fill in a time.  Assume about 2min / clip.\n",
    "test_time_clips = test_time[test_time['type'] == 'Clip'].replace(0,120000)['game_time']\n",
    "test_time.loc[test_time['type'] == 'Clip','game_time'] = test_time_clips \n",
    "\n",
    "# Then fill the rest of the events with 0 for those events \n",
    "# not associated with installation_ids or game_sessions where an assessment took place.  \n",
    "test_time = test_time.fillna(0)\n",
    "\n",
    "# Add accuracy_group and type to index so you can do a multi-level sort\n",
    "test_time = test_time.set_index(['game_session','assessment_id','type'],append=True)\n",
    "\n",
    "# Auto-dispatch the sum aggregation on multi-level group using the groupby method\n",
    "# Essentially sum the amount of time for each type of gameplay for a particular installation_id ->\n",
    "# world -> accuracy_group -> type of event to get an estimate of time spent in \n",
    "# each type of activity\n",
    "test_time_agg = test_time.groupby(level=['installation_id','world','assessment_id','game_session'])\\\n",
    "                .last()['game_time']\n",
    "\n",
    "\n",
    "test_time_agg = test_time['game_time'].groupby(level=['installation_id','world','assessment_id','type'])\\\n",
    "                                      .agg(game_time='sum')\n",
    "\n",
    "\n",
    "# Now unstack the type column to get counts for each type of activity with each \n",
    "# [installation_id, world] pair\n",
    "test_time = test_time_agg.unstack('type')\n",
    "del test_time_agg\n",
    "\n",
    "# Fill all [installation_id, world] rows that are missing an activity count with zeros\n",
    "test_time = test_time.fillna(0)\n",
    "\n",
    "test_time = test_time['game_time'].groupby(level=['installation_id','world'])\\\n",
    "                                        .transform(pd.DataFrame.cumsum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create arrays for the features and the response variable\n",
    "# X_final = test_time_scale.drop(['Assessment'], axis=1).values\n",
    "X_final = test_time.drop(['Assessment'], axis=1).values\n",
    "\n",
    "y_pred = knn.predict(X_final)\n",
    "\n",
    "# Coerce the predictions to ints since this is required for a \n",
    "# correct submission format.\n",
    "y_pred = y_pred.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put features and response variable into dataframe to be written to csv\n",
    "sub = pd.DataFrame({'installation_id':test_time.index.get_level_values(0).values, \n",
    "                    'accuracy_group':y_pred})\n",
    "                                         \n",
    "# Pick the last assessment score for each installation_id    \n",
    "sub = sub.groupby('installation_id').last()\n",
    "\n",
    "# Move installation_id out of the index as required\n",
    "# for the correct submission format.\n",
    "sub.reset_index(inplace=True)\n",
    "\n",
    "# Write to csv\n",
    "sub.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
